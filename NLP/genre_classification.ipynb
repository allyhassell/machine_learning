{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51655f1c-23d9-45d0-9dd9-16afa391f778",
   "metadata": {},
   "source": [
    "# DIGI405 Lab Class: Genre classification of historical newspaper texts\n",
    "\n",
    "In this notebook we will train and test logistic regression and Naive Bayes classifiers on a collection of texts from [historical New Zealand newspapers](https://paperspast.natlib.govt.nz/newspapers). Our aim is to build genre classification models that are independent of topic, so we will use features based on the structure and layout of the text (for example line widths), linguistic features (such as the frequency of certain parts-of-speech), and other text statistics.\n",
    "\n",
    "The data used in this notebook is originally sourced from the [National Library of New Zealand's Papers Past open data](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/dataset-papers-past-newspaper-open-data-pilot). It consists of a small dataset of articles that have been pre-labelled with their genre and includes features related to line widths and offsets that have been extracted from the [METS/ALTO XML files](https://veridiansoftware.com/knowledge-base/metsalto/) for each newspaper.\n",
    "\n",
    "We will use [spaCy](https://spacy.io/), [textfeatures](https://towardsdatascience.com/textfeatures-library-for-extracting-basic-features-from-text-data-f98ba90e3932), and [textstat](https://pypi.org/project/textstat/) to extract additional features and add them to our dataframe. We will then use [scikit-learn](https://scikit-learn.org/stable/) to train and test our models.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>\n",
    "\n",
    "![National Library Papers Past](https://images.ctfassets.net/pwv49hug9jad/58rs0U4wNbQAhTch5JeZsC/e22c8ac2acb9aa94698e8f084e136352/datasets-pp-open-data-feature-image.jpg?fm=webp)\n",
    "\n",
    "[Image source: natlib.govt.nz](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/dataset-papers-past-newspaper-open-data-pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c691409-29ef-40dd-9a0c-b6352d6418fb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-disclosure",
   "metadata": {},
   "source": [
    "We need to make sure the latest version of scikit-learn is installed (you only need to run this cell once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-volleyball",
   "metadata": {},
   "source": [
    "Now import the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d297294-06bf-41bf-a157-dd55226de21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Classifier training and evaluation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature extraction\n",
    "import spacy\n",
    "import textstat\n",
    "import textfeatures as tf\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,8)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-warrior",
   "metadata": {},
   "source": [
    "### Install Python packages if needed\n",
    "\n",
    "After running the first notebook cell you may see an import error telling you that you are missing certain modules such as textstat or seaborn. If this is the case run the cell below, and then re-run the cell above. Note the line(s) you want to run must be directly below the %%bash line i.e. with no commented lines between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f904c-3e38-453c-b3d9-b5ad0c8bd864",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m pip install textstat\n",
    "python -m pip install textfeatures\n",
    "python -m pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e1848-5064-46dc-b8fb-a90e41ead2c5",
   "metadata": {},
   "source": [
    "## Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb895d2-84af-4b02-9039-408685be95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "filepath = 'paperspast_4genres_labelled.csv'\n",
    "df = pd.read_csv(filepath, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0188bb-112f-40c4-a27c-d4778964dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the count of articles by genre\n",
    "display(df.groupby(['genre'])['genre'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554e5d4-b0e2-4165-9366-8394726b315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first ten rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the distribution of the articles in our dataset by newspaper\n",
    "\n",
    "sample_papers_unique = df['newspaper'].nunique()\n",
    "print(\"-----------------------------------------------------\") \n",
    "print(f\"Number of newspaper titles in sample dataset: {sample_papers_unique}\") \n",
    "print(\"-----------------------------------------------------\") \n",
    "print(\"\") \n",
    "\n",
    "ax_1 = sns.countplot(x=\"newspaper\", \n",
    "                     data = df, \n",
    "                     order = df['newspaper'].value_counts().index, \n",
    "                     color = \"#3949ab\")\n",
    "ax_1.set_xlabel(\"Newspaper\", fontsize = 14)\n",
    "ax_1.set_ylabel(\"Count of articles\", fontsize = 14)\n",
    "ax_1.set_title(\"Distribution of articles by newspaper\", fontsize = 16)\n",
    "plt.xticks(rotation = 90, fontsize = 13)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the distribution of the articles in our dataset by year\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "annual_df = (df.groupby([df['date'].dt.year.rename('year')])\n",
    "             ['text'].count().reset_index())\n",
    "\n",
    "ax_2 = sns.barplot(x=\"year\", y=\"text\", data=annual_df, color = '#3949ab')\n",
    "ax_2.set_xlabel(\"Year\", fontsize = 14)\n",
    "ax_2.set_ylabel(\"Count of articles\", fontsize = 14)\n",
    "ax_2.set_title(\"Distribution of articles in dataset by year\", fontsize = 16)\n",
    "plt.xticks(rotation = 90, fontsize = 13)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583aef-080f-427a-b883-6a7c7f633dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full text of a selected article by index\n",
    "selected_index = 250\n",
    "\n",
    "print(f\"\\nGenre: {df['genre'].values[selected_index]}\\n\")\n",
    "print(\"==============\\n\")\n",
    "print(f\"Title: {df['title'].values[selected_index]}\\n\")\n",
    "df['text'].values[selected_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe938d-098c-43a0-a6d2-525b66516574",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "You'll see from the above that there can be symbols and punctuation in the text that are the result of [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) errors. We will run a simple cleaner function over the text column of the dataframe to improve this and add the cleaned text to a new column. Before we remove punctuation, we will count the sentences and add this feature to the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10d6d-72cf-4383-b103-a1441004b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove unnecessary symbols to create a clean text column from the original dataframe column using a regex.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df['sentence_count'] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Regex pattern for only alphanumeric, hyphenated text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{1,50}\")\n",
    "    df['clean_text'] = df[column_name].str.findall(pattern).str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-values",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> Here we are doing a very simple clean-up of the text, but a number of OCR errors will remain, for example incorrect words such as 'oar' or 'onr' instead of 'our'. Think about the impact this might have on our model and discuss it with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a4177-4db9-4c31-aa96-f63252ea5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaner(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bbfbb-465d-4c63-9d76-5ac2ed780d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574a183-116d-46cf-ac2d-5aaab1b95b82",
   "metadata": {},
   "source": [
    "## Feature extraction: linguistic features and text statistics\n",
    "\n",
    "The following cells extract parts-of-speech and text statistic features and add them to the dataframe. For efficiency, the texts are [processed for parts-of-speech tagging](https://spacy.io/usage/processing-pipelines) as a stream using spaCy's [nlp.pipe](https://spacy.io/usage/processing-pipelines#processing). This allows the texts to be buffered in batches instead of one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ff582-a896-4423-9803-bcbba266d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe column containing the text to be processed\n",
    "text_col = 'clean_text'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385aa09-c042-43a7-afc7-6d9b5bb8f60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_propn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: proper nouns.\n",
    "    \"\"\"\n",
    "    count_propn = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            count_propn += 1\n",
    "        \n",
    "    return count_propn \n",
    "\n",
    "\n",
    "def count_verb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: verbs.\n",
    "    \"\"\"\n",
    "    count_verb = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            count_verb += 1\n",
    "\n",
    "    return count_verb\n",
    "\n",
    "\n",
    "def count_noun_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: nouns.\n",
    "    \"\"\"\n",
    "    count_noun = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            count_noun += 1\n",
    "\n",
    "    return count_noun\n",
    "\n",
    "\n",
    "def count_adj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \"\"\"\n",
    "    count_adj = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            count_adj += 1\n",
    "            \n",
    "    return count_adj\n",
    "\n",
    "\n",
    "def count_nums_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: numbers.\n",
    "    \"\"\" \n",
    "    count_nums = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            count_nums += 1\n",
    "        \n",
    "    return count_nums\n",
    "\n",
    "\n",
    "def count_pron_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return count of the \n",
    "    following parts-of-speech: pronouns.\n",
    "    \"\"\"\n",
    "    count_pron = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            count_pron += 1\n",
    "        \n",
    "    return count_pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797ee69-8406-4d0b-a157-a85e7dbbc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features_pipe(text_col, df):\n",
    "    \"\"\"\n",
    "    Process given text column of a dataframe to \n",
    "    extract linguistic features and add them to\n",
    "    the dataframe. Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_col = df[text_col]  \n",
    "\n",
    "    # spaCy processing pipeline\n",
    "    nlp_text_pipe = nlp.pipe(input_col, batch_size=20)\n",
    "    \n",
    "    propn_count = []\n",
    "    verb_count = []\n",
    "    noun_count = []\n",
    "    adj_count = []\n",
    "    nums_count = []\n",
    "    pron_count = []\n",
    "    \n",
    "    for doc in nlp_text_pipe:\n",
    "\n",
    "        # POS tags\n",
    "        # Universal POS Tags\n",
    "        # http://universaldependencies.org/u/pos/\n",
    "\n",
    "        # Count proper nouns\n",
    "        propn_total = count_propn_spacy(doc)\n",
    "        propn_count.append(propn_total)\n",
    "\n",
    "        # Count verbs\n",
    "        verb_total = count_verb_spacy(doc)\n",
    "        verb_count.append(verb_total)\n",
    "\n",
    "        # Count nouns\n",
    "        noun_total = count_noun_spacy(doc)\n",
    "        noun_count.append(noun_total)\n",
    "\n",
    "        # Count adjectives\n",
    "        adj_total = count_adj_spacy(doc)\n",
    "        adj_count.append(adj_total)\n",
    "\n",
    "        # Count numbers\n",
    "        nums_total = count_nums_spacy(doc)\n",
    "        nums_count.append(nums_total)\n",
    "\n",
    "        # Count pronouns\n",
    "        pron_total = count_pron_spacy(doc)\n",
    "        pron_count.append(pron_total)\n",
    "\n",
    "    # Add word and syllable counts using the textstat library \n",
    "    df['word_count'] = input_col.apply(lambda x: textstat.lexicon_count(x, removepunct=True)) \n",
    "    df['syll_count'] = input_col.apply(lambda x: textstat.syllable_count(x))\n",
    "    \n",
    "    # Add the number of words with a syllable count greater than or equal to 3\n",
    "    df['polysyll_count'] = input_col.apply(lambda x: textstat.polysyllabcount(x)) \n",
    "    \n",
    "    # Add the number of words with a syllable count equal to one\n",
    "    df['monosyll_count'] = input_col.apply(lambda x: textstat.monosyllabcount(x)) \n",
    "    \n",
    "    # Add stopwords count using the textfeatures library\n",
    "    tf.stopwords_count(df,text_col,'stopwords_count')\n",
    "    \n",
    "    # tf.stopwords(df,text_col,'stopwords')  # Include a column that lists the stopwords found in the text\n",
    "    \n",
    "    # Add average word length and character counts using the textfeatures library \n",
    "    try:\n",
    "        tf.avg_word_length(df,text_col,'avg_word_length')\n",
    "    except:\n",
    "        df['avg_word_length'] = 0\n",
    "    \n",
    "    try:\n",
    "        tf.char_count(df,text_col,'char_count')\n",
    "    except:\n",
    "        df['char_count'] = 0\n",
    "        \n",
    "    # Add parts of speech counts to the dataframe\n",
    "    df['propn_count'] = propn_count\n",
    "    df['verb_count'] = verb_count\n",
    "    df['noun_count'] = noun_count\n",
    "    df['adj_count'] = adj_count\n",
    "    df['nums_count'] = nums_count\n",
    "    df['pron_count'] = pron_count\n",
    "    \n",
    "    # Add frequency columns   \n",
    "    df['propn_freq'] = df['propn_count']/df['word_count']\n",
    "    df['verb_freq'] = df['verb_count']/df['word_count']\n",
    "    df['noun_freq'] = df['noun_count']/df['word_count']\n",
    "    df['adj_freq'] = df['adj_count']/df['word_count']\n",
    "    df['nums_freq'] = df['nums_count']/df['word_count']\n",
    "    df['pron_freq'] = df['pron_count']/df['word_count']\n",
    "    \n",
    "    df['polysyll_freq'] = df['polysyll_count']/df['word_count']\n",
    "    df['monosyll_freq'] = df['monosyll_count']/df['word_count']\n",
    "    df['stopword_freq'] = df['stopwords_count']/df['word_count']\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62f4f9-4772-44f6-a9e6-b38dfbb71e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to extract text features and add them to the dataframe\n",
    "# This will take a little while to run\n",
    "df = text_features_pipe(text_col, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa9212-9ade-4b1a-8952-711f629290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few rows of the dataframe to see the features that have been added\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect descriptive statistics for our numerical data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-pound",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Why is exploratory data anlaysis (EDA) using techniques such as visualising the data and examining descriptive statistics important? What can it reveal? Discuss with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64148-818c-4c2f-98ee-8a5f2a0a0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the features and data types of the dataframe\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60196a-74ac-42bb-b833-b14b2f1501e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the text, newspaper name, newspaper id, genre, and clean_text columns to strings\n",
    "df['genre'] = (df['genre']).astype('string')\n",
    "df['text'] = (df['text']).astype('string')\n",
    "df['clean_text'] = (df['clean_text']).astype('string')\n",
    "df['newspaper_id'] = (df['newspaper_id']).astype('string')\n",
    "df['newspaper'] = (df['newspaper']).astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724c5f6-f6aa-4e4a-91d1-13169e5edd6f",
   "metadata": {},
   "source": [
    "## Specify features to include in the model\n",
    "\n",
    "* We now need to specify the features we want to include in our model, for example it makes sense to only include the parts-of-speech frequencies not the counts.\n",
    "* You can include or remove features from the model to explore the impact of different combinations of features on the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ffc25-d663-4cc0-bc37-3ff6e377e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include in the model \n",
    "# Place cursor in the text and press Ctrl + / to comment or uncomment the line\n",
    "\n",
    "features = [\"propn_freq\", \n",
    "            \"verb_freq\", \n",
    "            \"noun_freq\", \n",
    "            \"adj_freq\",\n",
    "            \"nums_freq\", \n",
    "            \"pron_freq\", \n",
    "            \"stopword_freq\", \n",
    "            \"avg_line_offset\", \n",
    "#             \"max_line_offset\", \n",
    "            \"avg_line_width\", \n",
    "#             \"min_line_width\", \n",
    "#             \"max_line_width\", \n",
    "#             \"line_width_range\", \n",
    "            \"polysyll_freq\", \n",
    "            \"monosyll_freq\", \n",
    "            \"sentence_count\", \n",
    "#             \"word_count\", \n",
    "            \"avg_word_length\", \n",
    "#             \"char_count\", \n",
    "            \n",
    "            # We will code our target genre as '1' and the others as '0'\n",
    "            # Do not remove this feature from the set\n",
    "            \"binary_class\"  \n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46cd83-e31d-4f1c-8673-890ee3649a9b",
   "metadata": {},
   "source": [
    "## Set the target genre\n",
    "\n",
    "* We will specify the genre we want to predict with the binary classifier. \n",
    "* The selected genre will be labelled as 1 in the binary classification model, with the other classes labelled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a85462-bb01-4cee-8246-d40db029a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from:\n",
    "# FamilyNotice     \n",
    "# Fiction          \n",
    "# LetterToEditor    \n",
    "# Poetry         \n",
    "\n",
    "target_genre = \"Poetry\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-peninsula",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Train and test classifiers for each of the four genres and take note of the results in a separate document. Which combination of genre and classifier achieved the best metrics and which was the worst? Discuss the results with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c3a6-3533-4464-bdfa-991f38b90ee5",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets\n",
    "\n",
    "* Run the cells below to split the data into train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34dd20-47d5-4d2e-890d-41b5d299c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(df, features, target_genre):\n",
    "    \"\"\"\n",
    "    Given the dataframe, features to include in the model,\n",
    "    and the target genre, split the data into \n",
    "    training and test sets and use the dataframe indices to \n",
    "    save the order of the split\n",
    "    \"\"\"\n",
    "    \n",
    "    df['binary_class'] = np.where(df['genre']== target_genre, 1, 0)\n",
    "    model_df = df.filter(features, axis=1)\n",
    "    indices = df.index.values\n",
    "\n",
    "    # Extract the explanatory variables in X and the target variable in y\n",
    "    y = model_df.binary_class.copy()\n",
    "    X = model_df.drop(['binary_class'], axis=1)\n",
    "\n",
    "    # Train test split \n",
    "    # Use the indices to save the order of the split.\n",
    "    # https://stackoverflow.com/questions/48947194/add-randomforestclassifier-predict-proba-results-to-original-dataframe\n",
    "    X_train, X_test, indices_train, indices_test = train_test_split(X, \n",
    "                                                                    indices, \n",
    "                                                                    test_size = .3,    # This value changes the proportion of data held out for the test set\n",
    "                                                                    random_state = 3)\n",
    "    \n",
    "    y_train, y_test = y[indices_train], y[indices_test]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, indices_train, indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e1958-eaf9-43d1-8c7c-a8c5c3ea5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_data(df, features, target_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c5d5c-a77d-41a2-ad85-50a3bb574951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139306fc-894f-453d-9b04-ab5417bd26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195a2c3-d6b1-4e3c-b612-012c519d3c25",
   "metadata": {},
   "source": [
    "## Train and test a logistic regression classifier \n",
    "* [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) is a binary classification method popular for its computational efficiency and interpretability.\n",
    "* Run the cells below to train and test a logistic regression classifier for our selected genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e518c7-006e-4c8f-abac-a7f978c091f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_binary(X_train, X_test, y_train, y_test, target_genre):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', LogisticRegression())]) \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy_result = accuracy_score(y_test, y_pred)\n",
    "    precision_result = precision_score(y_test, y_pred)\n",
    "    recall_result = recall_score(y_test, y_pred)\n",
    "    f1_result = f1_score(y_test, y_pred)\n",
    "    auroc_result = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Logistic Regression\")\n",
    "    print(f\"{target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "    print(f\"Accuracy = {accuracy_result:.3f}\")\n",
    "    print(f\"Precision = {precision_result:.3f}\")\n",
    "    print(f\"Recall = {recall_result:.3f}\")\n",
    "    print(f\"F1 Score = {f1_result:.3f}\")\n",
    "    print(f\"AUROC Score = {auroc_result:.3f}\")\n",
    "    \n",
    "    RocCurveDisplay.from_predictions(y_test, y_pred)\n",
    "    plt.title(\"AUROC: Logistic Regression\")\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Model coefficients (converted to odds)\")\n",
    "    print(f\"{target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    y_prob_train = pipe.predict_proba(X_train)\n",
    "    y_prob_test = pipe.predict_proba(X_test)\n",
    "    \n",
    "    # Get coefficients and convert from log odds to odds\n",
    "    # https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1\n",
    "    odds = np.exp(pipe.named_steps['clf'].coef_[0])\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68553bc-821a-45be-a43d-8f16bb863b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_lr(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test, odds = log_reg_binary(X_train, \n",
    "                                                                                X_test, \n",
    "                                                                                y_train, \n",
    "                                                                                y_test, \n",
    "                                                                                target_genre)\n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,'pred_train'] = y_pred_train\n",
    "    df_new.loc[indices_test,'pred_test'] = y_pred_test\n",
    "    df_new.loc[indices_train,'prob_0_train'] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,'prob_0_test'] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,'prob_1_train'] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,'prob_1_test'] = y_prob_test[:,1]   \n",
    "    \n",
    "    # We will construct a link that allows us to visit the newspaper on the Papers Past website\n",
    "    df_new[\"newspaper_web\"] = df_new[\"date\"].astype('string')\n",
    "    df_new[\"newspaper_web\"] = df_new[\"newspaper_web\"].str.replace('-','/')\n",
    "    df_new[\"newspaper_web\"] = \"https://paperspast.natlib.govt.nz/newspapers/\" \\\n",
    "                            + df_new[\"newspaper\"].str.replace(\n",
    "                            ' ',\n",
    "                            '-', \n",
    "                            regex = False).str.lower().str.replace(\n",
    "                            \"'\",                                                                            \n",
    "                            \"\", \n",
    "                            regex = False).str.replace(\n",
    "                            \".\", \n",
    "                            \"\", \n",
    "                            regex = False) \\\n",
    "                            + \"/\" \\\n",
    "                            + df_new[\"newspaper_web\"]\n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    # Create a dataframe of the coefficients and features \n",
    "    lr_odds_df = pd.DataFrame(odds, X_train.columns, columns=['coef (odds)']).sort_values(by='coef (odds)', ascending=False)\n",
    "    lr_odds_df['feature'] = lr_odds_df.index\n",
    "    \n",
    "    return df_new, lr_odds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fe472-694a-4fa9-861c-877eb7cb9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_df, lr_odds_df = genres_binary_lr(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test)\n",
    "\n",
    "# Explore the model coefficients\n",
    "display(lr_odds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b618-e940-44ef-8a9e-521fda5a50a6",
   "metadata": {},
   "source": [
    "### Interpreting the logistic regression model\n",
    "A benefit of logistic regression is that it is relatively easy to interpret compared to other classifiers. We can extract the coefficients of the features in the final model (using the 'coef_' attribute) to see which features were the strongest predictors of the positive class (in our case, the selected genre). \n",
    "\n",
    "The coefficients extracted using 'coef_' are the log odds that an observation belongs to the positive class. In order to interpret them, we convert them to standard odds. Odds greater than 1 are positive odds and can be interpreted as follows:\n",
    "\n",
    "**\"For every unit increase in {feature}, the odds that the observation is {positive class} are {coef (odds)} times greater than the odds that it is not {positive class} when all other variables are held constant.\"**\n",
    "\n",
    "Odds less than 1 are the negative coefficients i.e. the strongest predictors that the observation is not in the positive class. To describe them in a similar way to the above, we need to take 1/odds. For example:\n",
    "\n",
    "\"For every unit increase in {feature}, the odds that the observation **is not** {positive class} are {1 / coef (odds)} times greater than the odds that it **is** {positive class} when all other variables are held constant.\"\n",
    "\n",
    "When interpreting the model coefficients it is important to consider the influence of features that may be correlated with each other. These features will have similar predictive relationships to the outcome and therefore the sign and value of the coefficients should be interpreted with caution. \n",
    "\n",
    "You can read more about calculating and interpreting the coefficients of regression models in this [Towards Data Science](https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1) article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e66ee-a857-47e7-b128-2a4fa6555502",
   "metadata": {},
   "source": [
    "## Train and test a Naive Bayes classifier \n",
    "* [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) methods are also widely used for text classification and have been effective in many real-world applications such as spam filtering.\n",
    "* Run the cells below to train and test a Naive Bayes classifier for our selected genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b044212-1a92-4a46-b872-d476424594fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_binary(X_train, X_test, y_train, y_test, target_genre):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes model to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', GaussianNB())]) \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy_result = accuracy_score(y_test, y_pred)\n",
    "    precision_result = precision_score(y_test, y_pred)\n",
    "    recall_result = recall_score(y_test, y_pred)\n",
    "    f1_result = f1_score(y_test, y_pred)\n",
    "    auroc_result = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Naive Bayes\")\n",
    "    print(f\"{target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "    print(f\"Accuracy = {accuracy_result:.3f}\")\n",
    "    print(f\"Precision = {precision_result:.3f}\")\n",
    "    print(f\"Recall = {recall_result:.3f}\")\n",
    "    print(f\"F1 Score = {f1_result:.3f}\")\n",
    "    print(f\"AUROC Score = {auroc_result:.3f}\")\n",
    "    \n",
    "    RocCurveDisplay.from_predictions(y_test, y_pred)\n",
    "    plt.title(\"AUROC: Naive Bayes\")\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    y_prob_train = pipe.predict_proba(X_train)\n",
    "    y_prob_test = pipe.predict_proba(X_test)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265ec28-6ad9-4bb4-9b0c-284f494f2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_nb(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test = nb_binary(X_train, \n",
    "                                                                     X_test, \n",
    "                                                                     y_train, \n",
    "                                                                     y_test, \n",
    "                                                                     target_genre)\n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,'pred_train'] = y_pred_train\n",
    "    df_new.loc[indices_test,'pred_test'] = y_pred_test\n",
    "    df_new.loc[indices_train,'prob_0_train'] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,'prob_0_test'] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,'prob_1_train'] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,'prob_1_test'] = y_prob_test[:,1]    \n",
    "       \n",
    "    # We will construct a link that allows us to visit the newspaper on the Papers Past website\n",
    "    df_new[\"newspaper_web\"] = df_new[\"date\"].astype('string')\n",
    "    df_new[\"newspaper_web\"] = df_new[\"newspaper_web\"].str.replace('-','/')\n",
    "    df_new[\"newspaper_web\"] = \"https://paperspast.natlib.govt.nz/newspapers/\" \\\n",
    "                            + df_new[\"newspaper\"].str.replace(\n",
    "                            ' ',\n",
    "                            '-', \n",
    "                            regex = False).str.lower().str.replace(\n",
    "                            \"'\",                                                                            \n",
    "                            \"\", \n",
    "                            regex = False).str.replace(\n",
    "                            \".\", \n",
    "                            \"\", \n",
    "                            regex = False) \\\n",
    "                            + \"/\" \\\n",
    "                            + df_new[\"newspaper_web\"]\n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96195ad4-befc-4622-a836-131060d8416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_preds_df = genres_binary_nb(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3161c-ff18-4318-a6f2-9a0f5587d86d",
   "metadata": {},
   "source": [
    "## Inspect incorrectly classified texts\n",
    "\n",
    "We can explore which texts were incorrectly classified by the two models. Run the cell below to display dataframes of the misclassified texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644d8eb-0dcb-4d59-81cd-690205cfd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "lr_misclass = lr_preds_df.loc[(lr_preds_df[\"binary_class\"] != lr_preds_df[\"pred_test\"]) & (lr_preds_df[\"pred_test\"] >= 0)]\n",
    "lr_misclass = lr_misclass.filter([\"date\", \n",
    "                                  \"newspaper_id\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"article_id\", \n",
    "                                  \"title\", \n",
    "                                  \"text\", \n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"newspaper_web\"], axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Logistic Regression model (lr)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(lr_misclass)\n",
    "\n",
    "nb_misclass = nb_preds_df.loc[(nb_preds_df[\"binary_class\"] != nb_preds_df[\"pred_test\"]) & (nb_preds_df[\"pred_test\"] >= 0)]\n",
    "nb_misclass = nb_misclass.filter([\"date\", \n",
    "                                  \"newspaper_id\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"article_id\", \n",
    "                                  \"title\", \n",
    "                                  \"text\", \n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"newspaper_web\"], axis=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Naive Bayes model (nb)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(nb_misclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599ce01-c104-494b-8532-75f222572c7a",
   "metadata": {},
   "source": [
    "### Display the full text and newspaper web address of a selected misclassification by model and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cb5b8-6a6c-4d80-875b-b8482c49d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and index number of the misclassified text\n",
    "\n",
    "# Enter 'lr' or 'nb'\n",
    "model = 'lr'\n",
    "selected_index = 1\n",
    "\n",
    "##################################################################\n",
    "\n",
    "if model == 'lr':\n",
    "    print(f\"\\n{lr_misclass['title'].values[selected_index]}\\n\")\n",
    "    print(lr_misclass['text'].values[selected_index])\n",
    "    print(\"\\nView the scanned newspaper on the Papers Past website - use Ctrl + F to search for the article by title\")\n",
    "    print(lr_misclass['newspaper_web'].values[selected_index])\n",
    "elif model == 'nb':\n",
    "    print(f\"\\n{nb_misclass['title'].values[selected_index]}\\n\")\n",
    "    print(nb_misclass['text'].values[selected_index])\n",
    "    print(\"\\nView the scanned newspaper on the Papers Past website - use Ctrl + F to search for the article by title\")\n",
    "    print(nb_misclass['newspaper_web'].values[selected_index])\n",
    "else:\n",
    "    print(\"\\nPlease enter either 'lr' or 'nb' for the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-strand",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> Examine some of the misclassified texts. Why do you think they were misclassified? Do the coefficients of the logistic regression model provide any clues? Discuss with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-newcastle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
